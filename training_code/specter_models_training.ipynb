{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries"
      ],
      "metadata": {
        "id": "w41GPUA-ILww"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "Y14i4-SZHbBu",
        "outputId": "bd8a2a16-b868-412c-f619-2528f83e4fea"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1-2115630580.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# For fine-tuning w/ HuggingFace API\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAutoModelForSequenceClassification\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataCollatorWithPadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   2043\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2044\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2045\u001b[0;31m                 \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2046\u001b[0m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2047\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mModuleNotFoundError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   2071\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2072\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2073\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2074\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2075\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/data/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m )\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mglue_compute_metrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxnli_compute_metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m from .processors import (\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0mDataProcessor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mInputExample\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/data/processors/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# limitations under the License.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mglue\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mglue_convert_examples_to_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglue_output_modes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglue_processors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglue_tasks_num_labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0msquad\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSquadExample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSquadFeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSquadV1Processor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSquadV2Processor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msquad_convert_examples_to_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataProcessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mInputExample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mInputFeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSingleSentenceClassificationProcessor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/data/processors/glue.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_tf_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_logger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mautograph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbitwise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/_api/v2/compat/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mv1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mforward_compatibility_horizon\u001b[0m \u001b[0;31m# line: 125\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/_api/v2/compat/v1/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mautograph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbitwise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/_api/v2/compat/v1/compat/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mv1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mforward_compatibility_horizon\u001b[0m \u001b[0;31m# line: 125\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/_api/v2/compat/v1/compat/v1/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mragged\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mraw_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mresource_loader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msaved_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/_api/v2/compat/v1/raw_ops/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   1427\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen_sparse_csr_matrix_ops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparseMatrixZeros\u001b[0m \u001b[0;31m# line: 1309\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1428\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen_sparse_csr_matrix_ops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSparseTensorToCSRSparseMatrix\u001b[0m \u001b[0;31m# line: 1371\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1429\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtpu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen_xla_ops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConvertToCooTensor\u001b[0m \u001b[0;31m# line: 84\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1430\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtpu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen_xla_ops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConvertToListOfSparseCoreCooTensors\u001b[0m \u001b[0;31m# line: 199\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1431\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtpu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen_xla_ops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConvertToSparseCoreCsrWrappedCooTensor\u001b[0m \u001b[0;31m# line: 373\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/tpu/ops/gen_xla_ops.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   3335\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3337\u001b[0;31m \u001b[0mXlaSparseDenseMatmulGradWithSgdAndCsrInput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_export\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"raw_ops.XlaSparseDenseMatmulGradWithSgdAndCsrInput\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_raw_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxla_sparse_dense_matmul_grad_with_sgd_and_csr_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mto_raw_op\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m   5995\u001b[0m   f = types.FunctionType(f.__code__, f.__globals__, f.__name__, f.__defaults__,\n\u001b[1;32m   5996\u001b[0m                          f.__closure__)\n\u001b[0;32m-> 5997\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mkwarg_only\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5998\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5999\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/tf_export.py\u001b[0m in \u001b[0;36mkwarg_only\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mkwarg_only\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m   \u001b[0;34m\"\"\"A wrapper that throws away all non-kwarg arguments.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m   \u001b[0mf_argspec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_inspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetfullargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tensorflow/python/util/tf_inspect.py\u001b[0m in \u001b[0;36mgetfullargspec\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecorator_argspec\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_convert_maybe_argspec_to_fullargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecorator_argspec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_getfullargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/inspect.py\u001b[0m in \u001b[0;36mgetfullargspec\u001b[0;34m(func)\u001b[0m\n\u001b[1;32m   1363\u001b[0m         \u001b[0;31m# so we ensure that remains the case in 3.3+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m         sig = _signature_from_callable(func,\n\u001b[0m\u001b[1;32m   1366\u001b[0m                                        \u001b[0mfollow_wrapper_chains\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m                                        \u001b[0mskip_bound_arg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/inspect.py\u001b[0m in \u001b[0;36m_signature_from_callable\u001b[0;34m(obj, follow_wrapper_chains, skip_bound_arg, globals, locals, eval_str, sigcls)\u001b[0m\n\u001b[1;32m   2521\u001b[0m         \u001b[0;31m# If it's a pure Python function, or an object that is duck type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2522\u001b[0m         \u001b[0;31m# of a Python function (Cython functions, for instance), then:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2523\u001b[0;31m         return _signature_from_function(sigcls, obj,\n\u001b[0m\u001b[1;32m   2524\u001b[0m                                         \u001b[0mskip_bound_arg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskip_bound_arg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2525\u001b[0m                                         globals=globals, locals=locals, eval_str=eval_str)\n",
            "\u001b[0;32m/usr/lib/python3.11/inspect.py\u001b[0m in \u001b[0;36m_signature_from_function\u001b[0;34m(cls, func, skip_bound_arg, globals, locals, eval_str)\u001b[0m\n\u001b[1;32m   2374\u001b[0m         \u001b[0mkind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_POSITIONAL_ONLY\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mposonly_left\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0m_POSITIONAL_OR_KEYWORD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2375\u001b[0m         \u001b[0mannotation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mannotations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_empty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2376\u001b[0;31m         parameters.append(Parameter(name, annotation=annotation,\n\u001b[0m\u001b[1;32m   2377\u001b[0m                                     kind=kind))\n\u001b[1;32m   2378\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mposonly_left\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/inspect.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, kind, default, annotation)\u001b[0m\n\u001b[1;32m   2669\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_empty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannotation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_empty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2670\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2671\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_kind\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_ParameterKind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2672\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'value {kind!r} is not a valid Parameter.kind'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# For fine-tuning w/ HuggingFace API\n",
        "from datasets import Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding, TrainingArguments, Trainer\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Dataset"
      ],
      "metadata": {
        "id": "I2vxm5M_IP6B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"TAB_binaryLabel.csv\")\n",
        "df.head()"
      ],
      "metadata": {
        "id": "oUaobDchITGj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Encoding Using SPECTER"
      ],
      "metadata": {
        "id": "AkS98I9UInVa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\"...specifically use citations as a naturally occurring, inter-document incidental supervision signal indicating which documents are most related and formulate the signal into a triplet-loss pretraining objective. Unlike many prior works, at inference time, our model does not require any citation information.\"\n",
        "* https://papertohtml.org/paper?id=a3e4ceb42cbcd2c807d53aff90a8cb1f5ee3f031\n",
        "* Overall more up to date and proven better than SciBERT\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-JisFsurJ3Ce"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load SPECTER (encoder)\n",
        "from transformers import AutoModel\n",
        "\n",
        "model_name = 'allenai/specter'\n",
        "model = AutoModel.from_pretrained(model_name)\n",
        "model.eval() # inference mode"
      ],
      "metadata": {
        "id": "2mxRZeQUHc64"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Change pandas dataframe to Hugging Face dataset\n",
        "HF_df = Dataset.from_pandas(df.sample(500))"
      ],
      "metadata": {
        "id": "xJKvfGT7Hrz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Embed TAB\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "def encode_tab(example):\n",
        "    \"\"\"\n",
        "    input:: Hugging Face dataset\n",
        "    return:: embed TAB column to some ex) [0.123, -0.456, 0.789, ..., 0.001]  # Shape: (768,)\n",
        "    NOTE: extra work is because we loaded BERT & BERT variations as end-to-end models (aka including classification) while SPECTER is embedding only\n",
        "    \"\"\"\n",
        "    input_text = example[\"TAB\"]\n",
        "    inputs = tokenizer(input_text, return_tensors = \"pt\", truncation = True, max_length = 512) # Dict {'input_ids': tensor, 'token_type_ids': tensor}\n",
        "    with torch.no_grad(): # disables gradient tracking...we are doing inference only, not training --> no need for backprop & storing gradient\n",
        "        outputs = model(**inputs) # ** unpacks the dictionary\n",
        "        cls_emb = outputs.last_hidden_state[:, 0, :]  # extract CLS token...[:, 0, :] all items, first item, all items\n",
        "    return {\"embedding\": cls_emb.squeeze().numpy()}"
      ],
      "metadata": {
        "id": "6KnK1ZzsHvbN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Features, Sequence, Value\n",
        "\n",
        "# Add 'embedding' feature as a sequence of floats\n",
        "features = HF_df.features.copy()\n",
        "features[\"embedding\"] = Sequence(Value(\"float32\"))\n",
        "\n",
        "tokenized_df = HF_df.map(\n",
        "    encode_tab,\n",
        "    features = features,\n",
        "    batched = False\n",
        ")"
      ],
      "metadata": {
        "id": "GcvZguwTHyZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example code to better see how the encoding works\n",
        "ex = HF_df['TAB'][6] # random example\n",
        "ex # string\n",
        "\n",
        "ex_token = tokenizer(HF_df['TAB'][6], return_tensors = \"pt\", truncation = True, max_length = 512)\n",
        "ex_token['input_ids'].type # dict of shape {'input_ids': Tensor, 'attention_mask': Tensor}\n",
        "\n",
        "ex_out = model(**ex_token)\n",
        "ex_out #'BaseModelOutputWithPoolingAndCrossAttentions' object w/ attributes\n",
        "\n",
        "ex_embedding = ex_out.last_hidden_state[:, 0, :] # ex.out.last_hidden_state has torch.Size([1, 220, 768])\n",
        "ex_embedding.squeeze().shape # torch.Size([768]), in code grad is disabled so it can be converted to a numpy array"
      ],
      "metadata": {
        "id": "FERJha5wH_eo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature Analysis"
      ],
      "metadata": {
        "id": "Yl9t317COmdy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "try and see the dimensionality of the dataset based on the specter embeddings which are 768 dimension"
      ],
      "metadata": {
        "id": "dtrN7R6JO-IE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA"
      ],
      "metadata": {
        "id": "qV16nMgO7ptL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot cumulative variance explained over # features\n",
        "pca = PCA().fit(tokenized_df[\"embedding\"])\n",
        "\n",
        "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
        "plt.xlabel('number of components')\n",
        "plt.ylabel('cumulative explained variance')"
      ],
      "metadata": {
        "id": "6c1K85d19Mn-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components = 3)\n",
        "vis_dims = pca.fit_transform(tokenized_df[\"embedding\"])\n",
        "df[\"PCA\"] = vis_dims.tolist() # make PCs a column in df\n",
        "\n",
        "import plotly.graph_objs as go # makes it interactive\n",
        "\n",
        "fig = go.Figure()\n",
        "\n",
        "labels = df[\"label\"]\n",
        "for i, label in enumerate(labels):\n",
        "  sub_matrix = np.array(df[df[\"label\"] == label][\"PCA\"].tolist())\n",
        "  x = sub_matrix[:, 0]\n",
        "  y = sub_matrix[:, 1]\n",
        "  z = sub_matrix[:, 2]\n",
        "\n",
        "  fig.add_trace(\n",
        "      go.Scatter3d(\n",
        "          x = x,\n",
        "          y = y,\n",
        "          z = z,\n",
        "          mode = \"markers\",\n",
        "          marker = dict(size = 5, color = i, colorscale = \"Viridis\", opacity = 0.8),\n",
        "          name = label\n",
        "      )\n",
        "  )\n",
        "\n",
        "fig.update_layout(\n",
        "    autosize = False,\n",
        "    title = \"3D Scatter Plot of Cateogries\",\n",
        "    width = 800,\n",
        "    height = 500,\n",
        "    margin = dict(l = 50, r = 50, b = 100, t = 100, pad = 10),\n",
        "    scene = dict(\n",
        "        xaxis = dict(title = \"x\"),\n",
        "        yaxis = dict(title = \"y\"),\n",
        "        zaxis = dict(title = \"z\"),\n",
        "    )\n",
        ")\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "f5ioRmJOO9Yn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Linear Model"
      ],
      "metadata": {
        "id": "N6oR27QhKvC5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It may better mimic the decision making process of scientists if it is very simple...breaking it up into whether key words are included/not"
      ],
      "metadata": {
        "id": "aBHyH-DkK7-l"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HKFEOcF7ICi-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Networks"
      ],
      "metadata": {
        "id": "hqCY66SzLd53"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "May better capture the higher dimensions of data...I think a more qualitative description of the process may be nice and thus can be built to be reflected in the layers like\n",
        "\n",
        "\n",
        "1.   Largest overview/checking for this one quality\n",
        "2.   Now I check in greater detail in this quality\n",
        "* idea is to mimic the structure of how layers in CNNs reflect steps in human vision and V1 --> IT\n",
        "\n",
        "Presume an embedder which returns the knowledge of a scientific reader...the info of the title and abstract are now captured, now its time to start splitting things up!\n"
      ],
      "metadata": {
        "id": "V2NsTJIPLj8t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Wendi's Code...play around to add more layers between embedding and classification DUH"
      ],
      "metadata": {
        "id": "b4lcblAIKFYE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text"
      ],
      "metadata": {
        "id": "m8_IgFFEK6rr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split df into train, val, and test\n",
        "df[\"embedding\"] = tokenized_df[\"embedding\"] # store SPECTER embeddings in df\n",
        "\n",
        "train_df, tmp_df = train_test_split(\n",
        "    df, test_size=0.20, random_state=42, stratify=df['label']\n",
        ")\n",
        "val_df, test_df = train_test_split(\n",
        "    tmp_df, test_size=0.50, random_state=42, stratify=tmp_df['label']\n",
        ")\n",
        "print(f\"Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")"
      ],
      "metadata": {
        "id": "jd3udXpJNz4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate class weights\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "class_weights_array = class_weight.compute_class_weight(\n",
        "    'balanced',\n",
        "    classes=np.unique(train_df['label']),\n",
        "    y=train_df['label']\n",
        ")\n",
        "class_weights = {i : class_weights_array[i] for i in range(len(class_weights_array))}\n",
        "\n",
        "print(\"Calculated Class Weights to counter imbalance:\")\n",
        "print(class_weights)"
      ],
      "metadata": {
        "id": "DbMd0RJUQuGn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32\n",
        "AUTOTUNE   = tf.data.AUTOTUNE\n",
        "\n",
        "def df_to_dataset(dataframe, shuffle=True):\n",
        "   # Convert list of embedding arrays to a single 2D NumPy array\n",
        "    embeddings = np.stack(dataframe['embedding'].values)\n",
        "    labels = dataframe['label'].values\n",
        "\n",
        "    ds = tf.data.Dataset.from_tensor_slices((embeddings, labels))\n",
        "\n",
        "    if shuffle:\n",
        "        ds = ds.shuffle(buffer_size=len(dataframe))\n",
        "    return ds.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
        "\n",
        "train_ds = df_to_dataset(train_df, shuffle=True)\n",
        "val_ds   = df_to_dataset(val_df,   shuffle=False)\n",
        "test_ds  = df_to_dataset(test_df,  shuffle=False)"
      ],
      "metadata": {
        "id": "koM6IMLPOeIK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers, Input, Model\n",
        "\n",
        "# Define input for precomputed embeddings\n",
        "embedding_dim = 768\n",
        "embedding_input = Input(shape=(embedding_dim,), dtype=tf.float32, name='embedding_input') # SPECTER embeddings\n",
        "\n",
        "# Apply dropout to prevent overfitting\n",
        "x = layers.Dropout(0.2)(embedding_input)\n",
        "\n",
        "\"\"\" TODO: PLAY AROUND HERE\n",
        "the idea here is that the layers and how much is \"cut\" each layer reflects some portion of the scientific review process\n",
        "\"\"\"\n",
        "\n",
        "# Add a hidden Dense layer\n",
        "x = layers.Dense(256, activation='relu')(x)\n",
        "x = layers.BatchNormalization()(x)\n",
        "x = layers.Dense(64, activation='relu')(x)\n",
        "\n",
        "\"\"\" END OF FOOLISHNESS \"\"\"\n",
        "\n",
        "# Output layer for binary classification\n",
        "logits = layers.Dense(1, activation='sigmoid', name='classifier')(x)\n",
        "\n",
        "model = Model(inputs=embedding_input, outputs=logits)\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(3e-5),\n",
        "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "    metrics=[\n",
        "        tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
        "        tf.keras.metrics.Precision(name='precision'),\n",
        "        tf.keras.metrics.Recall(name='recall'),\n",
        "    ]\n",
        ")\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "NQD6uZODOjq2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# EarlyStopping to prevent overfitting and restore best weights based on val_recall\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_precision', # should I change to precision\n",
        "    mode='max',\n",
        "    patience=2, # Wait 2 epochs for improvement, can just switch to 1 bc atp if it starts overfitting it doesn't improve\n",
        "    verbose=1,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath='checkpoint_model.keras',\n",
        "    monitor='val_recall',\n",
        "    mode='max',\n",
        "    save_best_only=True,\n",
        "    save_weights_only=False,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "print(\"\\nStarting model training...\")\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs= 10, # Increase epochs; EarlyStopping will find the best one\n",
        "    # Added class weights so recall is higher\n",
        "    class_weight=class_weights,\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "nfy4NoaoNdXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, precision_recall_curve\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Unfixed and untested finding threshold code\n",
        "\n",
        "# 0.5 eval\n",
        "print(\"\\n--- Evaluation on Test Set (Standard 0.5 Threshold) ---\")\n",
        "loss, acc, precision, recall = model.evaluate(test_ds)\n",
        "print(f\"Test Recall (at 0.5 threshold): {recall:.4f}\")\n",
        "\n",
        "\n",
        "# val set threshold finding\n",
        "print(\"\\n--- Finding Optimal Threshold to Boost Recall ---\")\n",
        "y_val_pred_probs = model.predict(val_ds).ravel()\n",
        "y_val_true = val_df['label'].values\n",
        "\n",
        "precisions, recalls, thresholds = precision_recall_curve(y_val_true, y_val_pred_probs)\n",
        "\n",
        "# Find the threshold that gives us at least 95% recall\n",
        "try:\n",
        "    target_recall = 0.95\n",
        "    idx = np.min(np.where(recalls >= target_recall))\n",
        "    high_recall_threshold = thresholds[idx]\n",
        "    print(f\"Threshold found for >={target_recall*100}% recall: {high_recall_threshold:.4f}\")\n",
        "except ValueError:\n",
        "    best_recall_idx = np.argmax(recalls)\n",
        "    high_recall_threshold = thresholds[best_recall_idx]\n",
        "    print(f\"Could not reach {target_recall*100}% recall. Using best possible recall threshold: {high_recall_threshold:.4f}\")\n",
        "\n",
        "\n",
        "# evaluate on the test set using our new threshold\n",
        "print(f\"\\n--- Final Evaluation on Test Set (Threshold: {high_recall_threshold:.4f}) ---\")\n",
        "y_test_pred_probs = model.predict(test_ds).ravel()\n",
        "y_test_true = test_df['label'].values\n",
        "y_test_pred_final = (y_test_pred_probs >= high_recall_threshold).astype(int)\n",
        "\n",
        "# class report and confusion matrix\n",
        "print(\"\\nClassification Report (Test Set):\")\n",
        "print(classification_report(y_test_true, y_test_pred_final, target_names=['Irrelevant (0)', 'Relevant (1)']))\n",
        "\n",
        "import seaborn as sns\n",
        "print(\"\\nConfusion Matrix (Test Set):\")\n",
        "cm = confusion_matrix(y_test_true, y_test_pred_final)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Irrelevant', 'Relevant'], yticklabels=['Irrelevant', 'Relevant'])\n",
        "plt.ylabel('Actual')\n",
        "plt.xlabel('Predicted')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "\n",
        "# Save model\n",
        "OUTPUT_DIR = 'bert_128tokens_recall_unweighted_reg.keras'\n",
        "model.save(OUTPUT_DIR)\n",
        "print(f\"\\nModel & tokenizer saved to {OUTPUT_DIR}\")"
      ],
      "metadata": {
        "id": "RezjPXgOTaDs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}